{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "emotive_inference.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN2WJ5P5Cr5o5//F4XxSsyB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepaliVerma/personal_files/blob/main/emotive_inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSdDf5lAMY-G"
      },
      "source": [
        "https://colab.research.google.com/drive/1bC8_izE3enGNjP71xb-GhTk0joDUvqtc#scrollTo=EsVvOca72H9t"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckhefCmnDOBS",
        "outputId": "c04a80ba-9a1f-4da2-e081-cafe06ed3510"
      },
      "source": [
        "# # Linking Google drive to use preprocessed data \n",
        "# from google.colab import drive\n",
        "\n",
        "# # This will prompt for authorization.\n",
        "# drive.mount('/content/drive')\n",
        "# #/content/drive/My Drive//"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iWhe_N9SD1HB",
        "outputId": "5671f321-c0df-4746-bfce-7129b4c729d3"
      },
      "source": [
        "!git clone https://github.com/Tandon-A/emotic.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'emotic'...\n",
            "remote: Enumerating objects: 329, done.\u001b[K\n",
            "remote: Counting objects: 100% (74/74), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 329 (delta 65), reused 46 (delta 46), pack-reused 255\u001b[K\n",
            "Receiving objects: 100% (329/329), 48.56 MiB | 25.83 MiB/s, done.\n",
            "Resolving deltas: 100% (188/188), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0xN1NNAFbqE",
        "outputId": "ef08a2c2-79f5-4e39-d60b-44c2b7e7ee85"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34memotic\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naWyJ0_DFgIQ",
        "outputId": "c406b27a-a809-44fa-b05d-ed5f88f7d68e"
      },
      "source": [
        "cd emotic/"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/emotic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "daXeN9ArFtC9"
      },
      "source": [
        "#download weights and place model weights on 'model/' folder\n",
        "#https://drive.google.com/drive/folders/1e-JLA7V73CQD5pjTFCSWnKCmB0gCpV1D?usp=sharing"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtYIQMiODXpj"
      },
      "source": [
        "import argparse \n",
        "import cv2\n",
        "import numpy as np \n",
        "import os \n",
        "\n",
        "import torch \n",
        "from torchvision import transforms\n",
        "\n",
        "from emotic import Emotic \n",
        "from inference import infer\n",
        "from yolo_utils import prepare_yolo, rescale_boxes, non_max_suppression"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlkEh4HXDlE5"
      },
      "source": [
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument('--gpu', type=int, default=0, help='gpu id')\n",
        "    parser.add_argument('--experiment_path', type=str, required=True, help='Path of experiment files (results, models, logs)')\n",
        "    parser.add_argument('--model_dir', type=str, default='models', help='Folder to access the models')\n",
        "    parser.add_argument('--result_dir', type=str, default='results', help='Path to save the results')\n",
        "    parser.add_argument('--inference_file', type=str, help='Text file containing image context paths and bounding box')\n",
        "    parser.add_argument('--video_file', type=str, help='Test video file')\n",
        "    # Generate args\n",
        "    args = parser.parse_args()\n",
        "    return args\n",
        "\n",
        "\n",
        "def get_bbox(yolo_model, device, image_context, yolo_image_size=416, conf_thresh=0.8, nms_thresh=0.4):\n",
        "  ''' Use yolo to obtain bounding box of every person in context image. \n",
        "  :param yolo_model: Yolo model to obtain bounding box of every person in context image. \n",
        "  :param device: Torch device. Used to send tensors to GPU (if available) for faster processing. \n",
        "  :yolo_image_size: Input image size for yolo model. \n",
        "  :conf_thresh: Confidence threshold for yolo model. Predictions with object confidence > conf_thresh are returned. \n",
        "  :nms_thresh: Non-maximal suppression threshold for yolo model. Predictions with IoU > nms_thresh are returned. \n",
        "  :return: Numpy array of bounding boxes. Array shape = (no_of_persons, 4). \n",
        "  '''\n",
        "  test_transform = transforms.Compose([transforms.ToPILImage(),transforms.ToTensor()])\n",
        "  image_yolo = test_transform(cv2.resize(image_context, (416, 416))).unsqueeze(0).to(device)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    detections = yolo_model(image_yolo)\n",
        "    nms_det  = non_max_suppression(detections, conf_thresh, nms_thresh)[0]\n",
        "    det = rescale_boxes(nms_det, yolo_image_size, (image_context.shape[:2]))\n",
        "  \n",
        "  bboxes = []\n",
        "  for x1, y1, x2, y2, _, _, cls_pred in det:\n",
        "    if cls_pred == 0:  # checking if predicted_class = persons. \n",
        "      x1 = int(min(image_context.shape[1], max(0, x1)))\n",
        "      x2 = int(min(image_context.shape[1], max(x1, x2)))\n",
        "      y1 = int(min(image_context.shape[0], max(15, y1)))\n",
        "      y2 = int(min(image_context.shape[0], max(y1, y2)))\n",
        "      bboxes.append([x1, y1, x2, y2])\n",
        "  return np.array(bboxes)\n",
        "\n",
        "\n",
        "def yolo_infer(images_list, result_path, model_path, context_norm, body_norm, ind2cat, ind2vad, args):\n",
        "  ''' Infer on a list of images defined in images_list text file to obtain bounding boxes of persons in the images using yolo model.\n",
        "  :param images_list: Text file specifying the images to conduct inference. A row in the file is Path_of_image. \n",
        "  :param result_path: Directory path to save the results (images with the predicted emotion categories and continuous emotion dimesnions).\n",
        "  :param model_path: Directory path to load models and val_thresholds to perform inference.\n",
        "  :param context_norm: List containing mean and std values for context images. \n",
        "  :param body_norm: List containing mean and std values for body images. \n",
        "  :param ind2cat: Dictionary converting integer index to categorical emotion. \n",
        "  :param ind2vad: Dictionary converting integer index to continuous emotion dimension (Valence, Arousal and Dominance).\n",
        "  :param args: Runtime arguments.\n",
        "  '''\n",
        "  device = torch.device(\"cuda:%s\" %(str(args.gpu)) if torch.cuda.is_available() else \"cpu\")\n",
        "  yolo = prepare_yolo(model_path)\n",
        "  yolo = yolo.to(device)\n",
        "  yolo.eval()\n",
        "\n",
        "  thresholds = torch.FloatTensor(np.load(os.path.join(result_path, 'val_thresholds.npy'))).to(device) \n",
        "  model_context = torch.load(os.path.join(model_path,'model_context1.pth')).to(device)\n",
        "  model_body = torch.load(os.path.join(model_path,'model_body1.pth')).to(device)\n",
        "  emotic_model = torch.load(os.path.join(model_path,'model_emotic1.pth')).to(device)\n",
        "  models = [model_context, model_body, emotic_model]\n",
        "\n",
        "  with open(images_list, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "  \n",
        "  for idx, line in enumerate(lines):\n",
        "    image_context_path = line.split('\\n')[0].split(' ')[0]\n",
        "    image_context = cv2.cvtColor(cv2.imread(image_context_path), cv2.COLOR_BGR2RGB)\n",
        "    try:\n",
        "      bbox_yolo = get_bbox(yolo, device, image_context)\n",
        "      for pred_bbox in bbox_yolo:\n",
        "        pred_cat, pred_cont = infer(context_norm, body_norm, ind2cat, ind2vad, device, thresholds, models, image_context=image_context, bbox=pred_bbox, to_print=False)\n",
        "        write_text_vad = list()\n",
        "        for continuous in pred_cont:\n",
        "          write_text_vad.append(str('%.1f' %(continuous)))\n",
        "        write_text_vad = 'vad ' + ' '.join(write_text_vad) \n",
        "        image_context = cv2.rectangle(image_context, (pred_bbox[0], pred_bbox[1]),(pred_bbox[2] , pred_bbox[3]), (255, 0, 0), 3)\n",
        "        cv2.putText(image_context, write_text_vad, (pred_bbox[0], pred_bbox[1] - 5), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
        "        for i, emotion in enumerate(pred_cat):\n",
        "          cv2.putText(image_context, emotion, (pred_bbox[0], pred_bbox[1] + (i+1)*12), cv2.FONT_HERSHEY_PLAIN, 1, (255, 255, 255), 1)\n",
        "    except Exception as e:\n",
        "      print ('Exception for image ',image_context_path)\n",
        "      print (e)\n",
        "    cv2.imwrite(os.path.join(result_path, 'img_%r.jpg' %(idx)), cv2.cvtColor(image_context, cv2.COLOR_RGB2BGR))\n",
        "    print ('completed inference for image %d'  %(idx))\n",
        "\n",
        "\n",
        "def yolo_video(video_file, result_path, model_path, context_norm, body_norm, ind2cat, ind2vad):\n",
        "  ''' Perform inference on a video. First yolo model is used to obtain bounding boxes of persons in every frame.\n",
        "  After that the emotic model is used to obtain categoraical and continuous emotion predictions. \n",
        "  :param video_file: Path of video file. \n",
        "  :param result_path: Directory path to save the results (output video).\n",
        "  :param model_path: Directory path to load models and val_thresholds to perform inference.\n",
        "  :param context_norm: List containing mean and std values for context images. \n",
        "  :param body_norm: List containing mean and std values for body images. \n",
        "  :param ind2cat: Dictionary converting integer index to categorical emotion. \n",
        "  :param ind2vad: Dictionary converting integer index to continuous emotion dimension (Valence, Arousal and Dominance).\n",
        "  :param args: Runtime arguments.\n",
        "  '''  \n",
        "  device = torch.device(\"cuda:%s\" %(str(gpu)) if torch.cuda.is_available() else \"cpu\")\n",
        "  yolo = prepare_yolo(model_path)\n",
        "  yolo = yolo.to(device)\n",
        "  yolo.eval()\n",
        "\n",
        "  thresholds = torch.FloatTensor(np.load(os.path.join(result_path, 'val_thresholds.npy'))).to(device) \n",
        "  model_context = torch.load(os.path.join(model_path,'model_context1.pth')).to(device)\n",
        "  model_body = torch.load(os.path.join(model_path,'model_body1.pth')).to(device)\n",
        "  emotic_model = torch.load(os.path.join(model_path,'model_emotic1.pth')).to(device)\n",
        "  model_context.eval()\n",
        "  model_body.eval()\n",
        "  emotic_model.eval()\n",
        "  models = [model_context, model_body, emotic_model]\n",
        "\n",
        "  video_stream = cv2.VideoCapture(video_file, cv2.CAP_FFMPEG)\n",
        "  writer = None\n",
        "\n",
        "  print ('Starting testing on video')\n",
        "  while True:\n",
        "    (grabbed, frame) = video_stream.read()\n",
        "    if not grabbed:\n",
        "      break\n",
        "    image_context = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    try: \n",
        "      bbox_yolo = get_bbox(yolo, device, image_context)\n",
        "      for pred_idx, pred_bbox in enumerate(bbox_yolo):\n",
        "        pred_cat, pred_cont = infer(context_norm, body_norm, ind2cat, ind2vad, device, thresholds, models, image_context=image_context, bbox=pred_bbox, to_print=False)\n",
        "        write_text_vad = list()\n",
        "        for continuous in pred_cont:\n",
        "          write_text_vad.append(str('%.1f' %(continuous)))\n",
        "        write_text_vad = 'vad ' + ' '.join(write_text_vad) \n",
        "        image_context = cv2.rectangle(image_context, (pred_bbox[0], pred_bbox[1]),(pred_bbox[2] , pred_bbox[3]), (255, 0, 0), 3)\n",
        "        cv2.putText(image_context, write_text_vad, (pred_bbox[0], pred_bbox[1] - 5), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)\n",
        "        for i, emotion in enumerate(pred_cat):\n",
        "          cv2.putText(image_context, emotion, (pred_bbox[0], pred_bbox[1] + (i+1)*12), cv2.FONT_HERSHEY_PLAIN, 1, (0, 0, 255), 2)\n",
        "    except Exception:\n",
        "      pass\n",
        "    if writer is None:\n",
        "      fourcc = cv2.VideoWriter_fourcc(*\"MJPG\")\n",
        "      writer = cv2.VideoWriter(os.path.join(result_path, 'result_vid.avi'), fourcc, 30, (image_context.shape[1], image_context.shape[0]), True)  \n",
        "    writer.write(cv2.cvtColor(image_context, cv2.COLOR_RGB2BGR))\n",
        "  writer.release()\n",
        "  video_stream.release() \n",
        "  print ('Completed video')\n",
        "\n",
        "\n",
        "def check_paths(args):\n",
        "  ''' Check (create if they don't exist) experiment directories.\n",
        "  :param args: Runtime arguments as passed by the user.\n",
        "  :return: result_dir_path, model_dir_path.\n",
        "  '''\n",
        "  if args.inference_file is not None: \n",
        "    if not os.path.exists(args.inference_file):\n",
        "      raise ValueError('inference file does not exist. Please pass a valid inference file')\n",
        "  if args.video_file is not None: \n",
        "    if not os.path.exists(args.video_file):\n",
        "      raise ValueError('video file does not exist. Please pass a valid video file')\n",
        "  if args.inference_file is None and args.video_file is None: \n",
        "    raise ValueError(' both inference file and video file can\\'t be none. Please specify one and run again')\n",
        "  model_path = os.path.join(args.experiment_path, args.model_dir)\n",
        "  if not os.path.exists(model_path):\n",
        "    raise ValueError('model path %s does not exist. Please pass a valid model_path' %(model_path))\n",
        "  result_path = os.path.join(args.experiment_path, args.result_dir)\n",
        "  if not os.path.exists(result_path):\n",
        "    os.makedirs(result_path)\n",
        "  return result_path, model_path\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bSHauxGDway"
      },
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfY_zO6aDnnd"
      },
      "source": [
        "if __name__=='__main__':\n",
        "  # args = parse_args()\n",
        "\n",
        "  # result_path, model_path = check_paths(args)\n",
        "  result_path = 'result/'\n",
        "  model_path = 'proj/data/emotic_pre/model/'\n",
        "  video_file = 'v_BoxingPunchingBag_g01_c02.avi'\n",
        "  gpu = 0\n",
        "\n",
        "  cat = ['Affection', 'Anger', 'Annoyance', 'Anticipation', 'Aversion', 'Confidence', 'Disapproval', 'Disconnection', \\\n",
        "          'Disquietment', 'Doubt/Confusion', 'Embarrassment', 'Engagement', 'Esteem', 'Excitement', 'Fatigue', 'Fear','Happiness', \\\n",
        "          'Pain', 'Peace', 'Pleasure', 'Sadness', 'Sensitivity', 'Suffering', 'Surprise', 'Sympathy', 'Yearning']\n",
        "  cat2ind = {}\n",
        "  ind2cat = {}\n",
        "  for idx, emotion in enumerate(cat):\n",
        "      cat2ind[emotion] = idx\n",
        "      ind2cat[idx] = emotion\n",
        "  \n",
        "  vad = ['Valence', 'Arousal', 'Dominance']\n",
        "  ind2vad = {}\n",
        "  for idx, continuous in enumerate(vad):\n",
        "      ind2vad[idx] = continuous\n",
        "  \n",
        "  context_mean = [0.4690646, 0.4407227, 0.40508908]\n",
        "  context_std = [0.2514227, 0.24312855, 0.24266963]\n",
        "  body_mean = [0.43832874, 0.3964344, 0.3706214]\n",
        "  body_std = [0.24784276, 0.23621225, 0.2323653]\n",
        "  context_norm = [context_mean, context_std]\n",
        "  body_norm = [body_mean, body_std]\n",
        "\n",
        "  # if inference_file is not None: \n",
        "  #   print ('inference over inference file images')\n",
        "  #   yolo_infer(inference_file, result_path, model_path, context_norm, body_norm, ind2cat, ind2vad)\n",
        "  if video_file is not None:\n",
        "    print ('inference over test video')\n",
        "    yolo_video(video_file, result_path, model_path, context_norm, body_norm, ind2cat, ind2vad)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}