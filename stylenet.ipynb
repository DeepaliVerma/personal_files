{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "stylenet.ipynb",
      "provenance": [],
      "mount_file_id": "1vTVfpPBtE2s_8J28Q9-2vxLJHFLS5HBe",
      "authorship_tag": "ABX9TyP3lMzL0NePHGhRy4kNqOGH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepaliVerma/personal_files/blob/main/stylenet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTuSpJrnfR9w"
      },
      "source": [
        "https://github.com/kacky24/stylenet/blob/master/data_loader.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NrU3iv-h558",
        "outputId": "60542cb5-fbb0-4ea8-b704-cbf8d19fb9c7"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FJ7ifSdbffu8",
        "outputId": "fae4aa65-6635-44bf-a06d-6e68ee80e9fb"
      },
      "source": [
        "import re\n",
        "import pickle\n",
        "from collections import Counter\n",
        "import nltk\n",
        "\n",
        "\n",
        "class Vocab:\n",
        "    '''vocabulary'''\n",
        "    def __init__(self):\n",
        "        self.w2i = {}\n",
        "        self.i2w = {}\n",
        "        self.ix = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        if word not in self.w2i:\n",
        "            self.w2i[word] = self.ix\n",
        "            self.i2w[self.ix] = word\n",
        "            self.ix += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        if word not in self.w2i:\n",
        "            return self.w2i['<unk>']\n",
        "        return self.w2i[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.w2i)\n",
        "\n",
        "\n",
        "def build_vocab(mode_list=['factual', 'humorous']):\n",
        "    '''build vocabulary'''\n",
        "    # define vocabulary\n",
        "    vocab = Vocab()\n",
        "    # add special tokens\n",
        "    vocab.add_word('<pad>')\n",
        "    vocab.add_word('<s>')\n",
        "    vocab.add_word('</s>')\n",
        "    vocab.add_word('<unk>')\n",
        "\n",
        "    # add words\n",
        "    for mode in mode_list:\n",
        "        if mode == 'factual':\n",
        "            captions = extract_captions(mode=mode)\n",
        "            words = nltk.tokenize.word_tokenize(captions)\n",
        "            counter = Counter(words)\n",
        "            words = [word for word, cnt in counter.items() if cnt >= 2]\n",
        "        else:\n",
        "            captions = extract_captions(mode=mode)\n",
        "            words = nltk.tokenize.word_tokenize(captions)\n",
        "\n",
        "        for word in words:\n",
        "            vocab.add_word(word)\n",
        "\n",
        "    return vocab\n",
        "\n",
        "# ls /content/drive/Shareddrives/BTP_group1/Flicker8k_Dataset/\n",
        "\n",
        "def extract_captions(mode='factual'):\n",
        "    '''extract captions from data files for building vocabulary'''\n",
        "    text = ''\n",
        "    if mode == 'factual':\n",
        "        with open(\"/content/drive/Shareddrives/BTP_group1/Flickr8k_text/Flickr8k.token.txt\", 'r') as f:\n",
        "            res = f.readlines()\n",
        "\n",
        "        r = re.compile(r'\\d*.jpg#\\d*')\n",
        "        for line in res:\n",
        "            line = r.sub('', line)\n",
        "            line = line.replace('.', '')\n",
        "            line = line.strip()\n",
        "            text += line + ' '\n",
        "# ls /content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/FlickrStyle_v0.9/romantic/romantic_train.txt\n",
        "    else:\n",
        "        if mode == 'humorous':\n",
        "            with open(\"/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/FlickrStyle_v0.9/humor/funny_train.txt\", 'r') as f:\n",
        "                res = f.readlines()\n",
        "        else:\n",
        "            with open(\"/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/FlickrStyle_v0.9/romantic/romantic_train.txt\", 'r') as f:\n",
        "                res = f.readlines()\n",
        "\n",
        "        for line in res:\n",
        "            line = line.replace('.', '')\n",
        "            line = line.strip()\n",
        "            text += line + ' '\n",
        "\n",
        "    return text.strip().lower()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    vocab = build_vocab(mode_list=['factual', 'humorous'])\n",
        "    print(vocab.__len__())\n",
        "    with open('/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/vocab.pkl', 'wb') as f:\n",
        "        pickle.dump(vocab, f)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "16244\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HT_MAyN4eLrG",
        "outputId": "46f7c974-d3a0-4b4e-b78e-2fe049d3c1c7"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import pickle\n",
        "import nltk\n",
        "import skimage.io\n",
        "import skimage.transform\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "# from build_vocab import Vocab\n",
        "\n",
        "\n",
        "class Flickr7kDataset(Dataset):\n",
        "    '''Flickr7k dataset'''\n",
        "    def __init__(self, img_dir, caption_file, vocab, transform=None):\n",
        "        '''\n",
        "        Args:\n",
        "            img_dir: Direcutory with all the images\n",
        "            caption_file: Path to the factual caption file\n",
        "            vocab: Vocab instance\n",
        "            transform: Optional transform to be applied\n",
        "        '''\n",
        "        self.img_dir = img_dir\n",
        "        # print(\"caption_file\", caption_file)\n",
        "        self.imgname_caption_list = self._get_imgname_and_caption(caption_file)\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def _get_imgname_and_caption(self, caption_file):\n",
        "        '''extract image name and caption from factual caption file'''\n",
        "        with open(caption_file, 'r') as f:\n",
        "            # print(\"f\",f)\n",
        "            res = f.readlines()\n",
        "            # print(\"res\", res)\n",
        "\n",
        "        imgname_caption_list = []\n",
        "        r = re.compile(r'#\\d*')\n",
        "        for line in res:\n",
        "            img_and_cap = r.split(line)\n",
        "            img_and_cap = [x.strip() for x in img_and_cap]\n",
        "            imgname_caption_list.append(img_and_cap)\n",
        "            # print(\"img_and_cap\", img_and_cap)\n",
        "        return imgname_caption_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgname_caption_list)\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        '''return one data pair (image and captioin)'''\n",
        "        img_name = self.imgname_caption_list[ix][0]\n",
        "        img_name = os.path.join(self.img_dir, img_name)\n",
        "        # print(\"self.imgname_caption_list:\", self.imgname_caption_list[ix][1])\n",
        "        caption = self.imgname_caption_list[ix][1]\n",
        "\n",
        "        image = skimage.io.imread(img_name)\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # convert caption to word ids\n",
        "        r = re.compile(\"\\.\")\n",
        "        tokens = nltk.tokenize.word_tokenize(r.sub(\"\", caption).lower())\n",
        "        caption = []\n",
        "        caption.append(self.vocab('<s>'))\n",
        "        caption.extend([self.vocab(token) for token in tokens])\n",
        "        caption.append(self.vocab('</s>'))\n",
        "        caption = torch.Tensor(caption)\n",
        "        return image, caption\n",
        "\n",
        "\n",
        "class FlickrStyle7kDataset(Dataset):\n",
        "    '''Styled caption dataset'''\n",
        "    def __init__(self, caption_file, vocab):\n",
        "        '''\n",
        "        Args:\n",
        "            caption_file: Path to styled caption file\n",
        "            vocab: Vocab instance\n",
        "        '''\n",
        "        self.caption_list = self._get_caption(caption_file)\n",
        "        self.vocab = vocab\n",
        "\n",
        "    def _get_caption(self, caption_file):\n",
        "        '''extract caption list from styled caption file'''\n",
        "        with open(caption_file, 'r') as f:\n",
        "            caption_list = f.readlines()\n",
        "\n",
        "        caption_list = [x.strip() for x in caption_list]\n",
        "        return caption_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.caption_list)\n",
        "\n",
        "    def __getitem__(self, ix):\n",
        "        caption = self.caption_list[ix]\n",
        "        # convert caption to word ids\n",
        "        r = re.compile(\"\\.\")\n",
        "        tokens = nltk.tokenize.word_tokenize(r.sub(\"\", caption).lower())\n",
        "        caption = []\n",
        "        caption.append(self.vocab('<s>'))\n",
        "        caption.extend([self.vocab(token) for token in tokens])\n",
        "        caption.append(self.vocab('</s>'))\n",
        "        caption = torch.Tensor(caption)\n",
        "        return caption\n",
        "\n",
        "\n",
        "def get_data_loader(img_dir, caption_file, vocab, batch_size,\n",
        "                    transform=None, shuffle=False, num_workers=0):\n",
        "    '''Return data_loader'''\n",
        "    if transform is None:\n",
        "        transform = transforms.Compose([\n",
        "            Rescale((224, 224)),\n",
        "            transforms.ToTensor()\n",
        "            ])\n",
        "    # print(\"caption_file\", caption_file)\n",
        "    flickr7k = Flickr7kDataset(img_dir, caption_file, vocab, transform)\n",
        "    # print(\"flickr7k\", flickr7k)\n",
        "    data_loader = DataLoader(dataset=flickr7k,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=collate_fn)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "def get_styled_data_loader(caption_file, vocab, batch_size,\n",
        "                           shuffle=False, num_workers=0):\n",
        "    '''Return data_loader for styled caption'''\n",
        "    flickr_styled_7k = FlickrStyle7kDataset(caption_file, vocab)\n",
        "\n",
        "    data_loader = DataLoader(dataset=flickr_styled_7k,\n",
        "                             batch_size=batch_size,\n",
        "                             shuffle=shuffle,\n",
        "                             num_workers=num_workers,\n",
        "                             collate_fn=collate_fn_styled)\n",
        "    return data_loader\n",
        "\n",
        "\n",
        "class Rescale:\n",
        "    '''Rescale the image to a given size\n",
        "    Args:\n",
        "        output_size(int or tuple)\n",
        "    '''\n",
        "    def __init__(self, output_size):\n",
        "        assert isinstance(output_size, (int, tuple))\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def __call__(self, image):\n",
        "        h, w = image.shape[:2]\n",
        "        if isinstance(self.output_size, int):\n",
        "            if h > w:\n",
        "                new_h, new_w = self.output_size * h / w, self.output_size\n",
        "            else:\n",
        "                new_h, new_w = self.output_size, self.output_size * w / h\n",
        "        else:\n",
        "            new_h, new_w = self.output_size\n",
        "\n",
        "        new_h, new_w = int(new_h), int(new_w)\n",
        "        image = skimage.transform.resize(image, (new_h, new_w))\n",
        "\n",
        "        return image\n",
        "\n",
        "\n",
        "def collate_fn(data):\n",
        "    '''create minibatch tensors from data(list of tuple(image, caption))'''\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    # images : tuple of 3D tensor -> 4D tensor\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # captions : tuple of 1D Tensor -> 2D tensor\n",
        "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
        "    captions = [pad_sequence(cap, max(lengths)) for cap in captions]\n",
        "    captions = torch.stack(captions, 0)\n",
        "\n",
        "    return images, captions, lengths\n",
        "\n",
        "\n",
        "def collate_fn_styled(captions):\n",
        "    captions.sort(key=lambda x: len(x), reverse=True)\n",
        "\n",
        "    # tuple of 1D Tensor -> 2D Tensor\n",
        "    lengths = torch.LongTensor([len(cap) for cap in captions])\n",
        "    captions = [pad_sequence(cap, max(lengths)) for cap in captions]\n",
        "    captions = torch.stack(captions, 0)\n",
        "\n",
        "    return captions, lengths\n",
        "\n",
        "\n",
        "def pad_sequence(seq, max_len):\n",
        "    seq = torch.cat((seq, torch.zeros(max_len - len(seq))))\n",
        "    return seq\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    with open(\"/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/vocab.pkl\", 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    img_path = \"/content/drive/Shareddrives/BTP_group1/Flicker8k_Dataset/\"\n",
        "    cap_path = \"/content/drive/Shareddrives/BTP_group1/Flickr8k_text/Flickr8k.token.txt\"\n",
        "    cap_path_styled = \"/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/FlickrStyle_v0.9/humor/funny_train.txt\"\n",
        "    data_loader = get_data_loader(img_path, cap_path, vocab, 3)\n",
        "    styled_data_loader = get_styled_data_loader(cap_path_styled, vocab, 3)\n",
        "\n",
        "    for i, (img, captions, lengths) in enumerate(data_loader):\n",
        "        print(\"i------->\",i)\n",
        "        # print(images.shape)\n",
        "        print(\"caption----------->\",captions[:, 1:])\n",
        "        print(\"length--------------->\",lengths - 1)\n",
        "        print()\n",
        "        if i == 3:\n",
        "            break\n",
        "\n",
        "    for i, (captions, lengths) in enumerate(styled_data_loader):\n",
        "        print(\"i------->\",i)\n",
        "        # print(images.shape)\n",
        "        print(\"caption----------->\",captions[:, 1:])\n",
        "        print(\"length--------------->\",lengths - 1)\n",
        "        print()\n",
        "        if i == 3:\n",
        "            break"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "i-------> 0\n",
            "caption-----------> tensor([[ 5.,  6.,  7.,  5.,  8.,  9., 10., 11., 12.,  5., 13., 14., 15.,  7.,\n",
            "         16.,  3., 17.,  2.],\n",
            "        [ 5., 23., 18., 11., 20.,  5., 21., 24.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.],\n",
            "        [ 5., 18., 19., 20.,  5., 21., 22.,  2.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.]])\n",
            "length---------------> tensor([18,  9,  8])\n",
            "\n",
            "i-------> 1\n",
            "caption-----------> tensor([[ 5., 23., 18.,  7.,  5.,  8.,  9., 19., 20.,  5., 21., 28.,  2.],\n",
            "        [ 5., 23., 18., 11., 25., 15., 26., 27., 24.,  2.,  0.,  0.,  0.],\n",
            "        [ 5., 30., 31., 32.,  5., 33., 31., 34., 35.,  2.,  0.,  0.,  0.]])\n",
            "length---------------> tensor([13, 10, 10])\n",
            "\n",
            "i-------> 2\n",
            "caption-----------> tensor([[ 5., 30., 31., 32.,  5., 43., 31., 38., 44., 45., 34., 46., 47., 39.,\n",
            "         40.,  7., 25., 48.,  2.],\n",
            "        [ 5., 30., 31., 32.,  5., 36., 31., 37., 38., 39., 40., 41., 25., 42.,\n",
            "          2.,  0.,  0.,  0.,  0.],\n",
            "        [49., 50., 14., 51., 52., 53., 47., 39., 40., 41., 25., 42.,  2.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.]])\n",
            "length---------------> tensor([19, 15, 13])\n",
            "\n",
            "i-------> 3\n",
            "caption-----------> tensor([[ 5., 23., 18., 58.,  7., 59., 60.,  7., 61., 14.,  5., 62., 63., 38.,\n",
            "         27., 64.,  7.,  5., 65.,  2.],\n",
            "        [ 5., 23., 18., 10., 66.,  7., 61., 14.,  5., 67., 62., 63.,  2.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.],\n",
            "        [49., 50., 41., 54., 55., 56., 39., 40.,  2.,  0.,  0.,  0.,  0.,  0.,\n",
            "          0.,  0.,  0.,  0.,  0.,  0.]])\n",
            "length---------------> tensor([20, 13,  9])\n",
            "\n",
            "i-------> 0\n",
            "caption-----------> tensor([[4.9000e+01, 5.0000e+01, 1.7010e+03, 3.9000e+01, 4.0000e+01, 5.1000e+02,\n",
            "         2.5000e+01, 3.6600e+02, 9.8000e+01, 7.0000e+00, 9.1580e+03, 1.4000e+01,\n",
            "         1.3160e+03, 1.3331e+04, 2.0000e+00],\n",
            "        [5.0000e+00, 3.1000e+01, 3.8000e+01, 1.0300e+02, 8.0000e+00, 7.0000e+00,\n",
            "         1.3800e+02, 2.3600e+02, 1.0000e+01, 5.3000e+01, 2.2080e+03, 2.6000e+01,\n",
            "         1.6000e+01, 1.3333e+04, 2.0000e+00],\n",
            "        [5.0000e+00, 2.3000e+01, 1.8000e+01, 7.0000e+01, 6.2730e+03, 9.1000e+01,\n",
            "         2.6000e+01, 5.0000e+00, 1.0460e+03, 2.6000e+01, 1.3332e+04, 2.7000e+01,\n",
            "         7.1390e+03, 2.0000e+00, 0.0000e+00]])\n",
            "length---------------> tensor([15, 15, 14])\n",
            "\n",
            "i-------> 1\n",
            "caption-----------> tensor([[5.0000e+00, 1.8000e+01, 5.0500e+02, 5.0000e+00, 5.8300e+02, 7.0000e+00,\n",
            "         2.5000e+01, 4.8000e+01, 2.4600e+02, 2.6000e+01, 2.7490e+03, 2.7000e+01,\n",
            "         1.3335e+04, 1.3336e+04, 2.0000e+00],\n",
            "        [5.0000e+00, 3.1000e+01, 3.1500e+02, 4.1000e+01, 2.5000e+01, 2.9900e+02,\n",
            "         1.3500e+02, 2.5000e+01, 1.4900e+02, 1.4100e+02, 5.9520e+03, 2.0300e+02,\n",
            "         1.3334e+04, 2.0000e+00, 0.0000e+00],\n",
            "        [5.0000e+00, 3.1000e+01, 1.0000e+01, 7.1950e+03, 8.5000e+01, 5.0000e+00,\n",
            "         1.1500e+02, 5.5300e+02, 2.3900e+02, 7.0000e+00, 2.5000e+01, 2.4000e+02,\n",
            "         2.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "length---------------> tensor([15, 14, 13])\n",
            "\n",
            "i-------> 2\n",
            "caption-----------> tensor([[5.0000e+00, 1.5500e+02, 1.0700e+02, 5.0000e+00, 1.1500e+02, 8.5200e+02,\n",
            "         1.0000e+01, 1.2400e+02, 1.2900e+02, 2.5000e+01, 7.1400e+02, 1.4100e+02,\n",
            "         2.4600e+02, 2.6000e+01, 2.3430e+03, 2.0000e+00],\n",
            "        [5.0000e+00, 1.6800e+02, 7.7000e+01, 1.8000e+01, 7.0000e+00, 8.5700e+02,\n",
            "         1.0000e+01, 3.7000e+01, 1.4200e+02, 3.8000e+01, 5.0000e+00, 1.0260e+03,\n",
            "         1.4000e+01, 3.9340e+03, 2.0000e+00, 0.0000e+00],\n",
            "        [5.0000e+00, 8.0000e+01, 1.0550e+03, 2.0300e+02, 2.5000e+01, 3.7900e+02,\n",
            "         1.4100e+02, 1.6800e+02, 3.8000e+01, 5.0000e+00, 1.8060e+03, 1.3337e+04,\n",
            "         2.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "length---------------> tensor([16, 15, 13])\n",
            "\n",
            "i-------> 3\n",
            "caption-----------> tensor([[5.0000e+00, 1.9300e+02, 2.4400e+02, 5.1600e+02, 7.0000e+00, 2.5000e+01,\n",
            "         2.4000e+02, 1.4400e+03, 5.0000e+00, 1.0197e+04, 3.8000e+01, 5.0000e+00,\n",
            "         3.9900e+02, 1.4000e+01, 2.5000e+01, 5.3800e+02, 2.0000e+00],\n",
            "        [5.0000e+00, 2.4900e+02, 7.0000e+00, 5.0000e+00, 1.1500e+02, 3.0700e+02,\n",
            "         1.0000e+01, 7.3510e+03, 5.0000e+00, 4.8620e+03, 5.4100e+02, 2.4600e+02,\n",
            "         2.6000e+01, 2.2900e+02, 5.0000e+00, 1.3338e+04, 2.0000e+00],\n",
            "        [5.0000e+00, 6.7000e+01, 2.0800e+02, 1.4000e+01, 4.0750e+03, 1.6300e+02,\n",
            "         7.0000e+00, 5.0000e+00, 9.8100e+02, 1.4400e+03, 5.0000e+00, 1.3339e+04,\n",
            "         1.4000e+01, 4.1480e+03, 2.0000e+00, 0.0000e+00, 0.0000e+00]])\n",
            "length---------------> tensor([17, 17, 15])\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNpe_Mlflzqd"
      },
      "source": [
        "SYMBOL_MAP = {\n",
        "        '<pad>': 0,\n",
        "        '<s>': 1,\n",
        "        '</s>': 2,\n",
        "        '<unk>': 3\n",
        "        }\n",
        "\n",
        "\n",
        "def get_symbol_id(symbol):\n",
        "    return SYMBOL_MAP[symbol]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzDibcnIlt9u"
      },
      "source": [
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "# from constant import get_symbol_id\n",
        "\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        '''\n",
        "        Load the pretrained ResNet152 and replace fc\n",
        "        '''\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True)\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "        self.A = nn.Linear(resnet.fc.in_features, emb_dim)\n",
        "\n",
        "    def forward(self, images):\n",
        "        '''Extract the image feature vectors'''\n",
        "        features = self.resnet(images)\n",
        "        features = Variable(features.data)\n",
        "        # if torch.cuda.is_available():\n",
        "        #     features = features.cuda()\n",
        "        features = features.view(features.size(0), -1)\n",
        "        features = self.A(features)\n",
        "        return features\n",
        "\n",
        "\n",
        "class FactoredLSTM(nn.Module):\n",
        "    def __init__(self, emb_dim, hidden_dim, factored_dim,  vocab_size):\n",
        "        super(FactoredLSTM, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # embedding\n",
        "        self.B = nn.Embedding(vocab_size, emb_dim)\n",
        "\n",
        "        # factored lstm weights\n",
        "        self.U_i = nn.Linear(factored_dim, hidden_dim)\n",
        "        self.S_fi = nn.Linear(factored_dim, factored_dim)\n",
        "        self.V_i = nn.Linear(emb_dim, factored_dim)\n",
        "        self.W_i = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.U_f = nn.Linear(factored_dim, hidden_dim)\n",
        "        self.S_ff = nn.Linear(factored_dim, factored_dim)\n",
        "        self.V_f = nn.Linear(emb_dim, factored_dim)\n",
        "        self.W_f = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.U_o = nn.Linear(factored_dim, hidden_dim)\n",
        "        self.S_fo = nn.Linear(factored_dim, factored_dim)\n",
        "        self.V_o = nn.Linear(emb_dim, factored_dim)\n",
        "        self.W_o = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.U_c = nn.Linear(factored_dim, hidden_dim)\n",
        "        self.S_fc = nn.Linear(factored_dim, factored_dim)\n",
        "        self.V_c = nn.Linear(emb_dim, factored_dim)\n",
        "        self.W_c = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.S_hi = nn.Linear(factored_dim, factored_dim)\n",
        "        self.S_hf = nn.Linear(factored_dim, factored_dim)\n",
        "        self.S_ho = nn.Linear(factored_dim, factored_dim)\n",
        "        self.S_hc = nn.Linear(factored_dim, factored_dim)\n",
        "\n",
        "        # self.S_ri = nn.Linear(factored_dim, factored_dim)\n",
        "        # self.S_rf = nn.Linear(factored_dim, factored_dim)\n",
        "        # self.S_ro = nn.Linear(factored_dim, factored_dim)\n",
        "        # self.S_rc = nn.Linear(factored_dim, factored_dim)\n",
        "\n",
        "        # weight for output\n",
        "        self.C = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward_step(self, embedded, h_0, c_0, mode):\n",
        "        i = self.V_i(embedded)\n",
        "        f = self.V_f(embedded)\n",
        "        o = self.V_o(embedded)\n",
        "        c = self.V_c(embedded)\n",
        "\n",
        "        if mode == \"factual\":\n",
        "            i = self.S_fi(i)\n",
        "            f = self.S_ff(f)\n",
        "            o = self.S_fo(o)\n",
        "            c = self.S_fc(c)\n",
        "        elif mode == \"humorous\":\n",
        "            i = self.S_hi(i)\n",
        "            f = self.S_hf(f)\n",
        "            o = self.S_ho(o)\n",
        "            c = self.S_hc(c)\n",
        "        # elif mode == \"romantic\":\n",
        "        #     i = self.S_ri(i)\n",
        "        #     f = self.S_rf(f)\n",
        "        #     o = self.S_ro(o)\n",
        "        #     c = self.S_rc(c)\n",
        "        else:\n",
        "            sys.stderr.write(\"mode name wrong!\")\n",
        "\n",
        "        i_t = F.sigmoid(self.U_i(i) + self.W_i(h_0))\n",
        "        f_t = F.sigmoid(self.U_f(f) + self.W_f(h_0))\n",
        "        o_t = F.sigmoid(self.U_o(o) + self.W_o(h_0))\n",
        "        c_tilda = F.tanh(self.U_c(c) + self.W_c(h_0))\n",
        "\n",
        "        c_t = f_t * c_0 + i_t * c_tilda\n",
        "        h_t = o_t * c_t\n",
        "\n",
        "        outputs = self.C(h_t)\n",
        "\n",
        "        return outputs, h_t, c_t\n",
        "\n",
        "    def forward(self, captions, features=None, mode=\"factual\"):\n",
        "        '''\n",
        "        Args:\n",
        "            features: fixed vectors from images, [batch, emb_dim]\n",
        "            captions: [batch, max_len]\n",
        "            mode: type of caption to generate\n",
        "        '''\n",
        "        batch_size = captions.size(0)\n",
        "        embedded = self.B(captions)  # [batch, max_len, emb_dim]\n",
        "        # concat features and captions\n",
        "        if mode == \"factual\":\n",
        "            if features is None:\n",
        "                sys.stderr.write(\"features is None!\")\n",
        "            embedded = torch.cat((features.unsqueeze(1), embedded), 1)\n",
        "\n",
        "        # initialize hidden state\n",
        "        h_t = Variable(torch.Tensor(batch_size, self.hidden_dim))\n",
        "        c_t = Variable(torch.Tensor(batch_size, self.hidden_dim))\n",
        "        nn.init.uniform(h_t)\n",
        "        nn.init.uniform(c_t)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            h_t = h_t.cuda()\n",
        "            c_t = c_t.cuda()\n",
        "\n",
        "        all_outputs = []\n",
        "        # iterate\n",
        "        for ix in range(embedded.size(1) - 1):\n",
        "            emb = embedded[:, ix, :]\n",
        "            outputs, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
        "            all_outputs.append(outputs)\n",
        "\n",
        "        all_outputs = torch.stack(all_outputs, 1)\n",
        "\n",
        "        return all_outputs\n",
        "\n",
        "    def sample(self, feature, beam_size=5, max_len=30, mode=\"factual\"):\n",
        "        '''\n",
        "        generate captions from feature vectors with beam search\n",
        "        Args:\n",
        "            features: fixed vector for an image, [1, emb_dim]\n",
        "            beam_size: stock size for beam search\n",
        "            max_len: max sampling length\n",
        "            mode: type of caption to generate\n",
        "        '''\n",
        "        # initialize hidden state\n",
        "        h_t = Variable(torch.Tensor(1, self.hidden_dim))\n",
        "        c_t = Variable(torch.Tensor(1, self.hidden_dim))\n",
        "        nn.init.uniform(h_t)\n",
        "        nn.init.uniform(c_t)\n",
        "\n",
        "        # if torch.cuda.is_available():\n",
        "        #     h_t = h_t.cuda()\n",
        "        #     c_t = c_t.cuda()\n",
        "\n",
        "        # forward 1 step\n",
        "        _, h_t, c_t = self.forward_step(feature, h_t, c_t, mode=mode)\n",
        "\n",
        "        # candidates: [score, decoded_sequence, h_t, c_t]\n",
        "        symbol_id = torch.LongTensor([1]).unsqueeze(0)\n",
        "        symbol_id = Variable(symbol_id, volatile=True)\n",
        "        # if torch.cuda.is_available():\n",
        "        #     symbol_id = symbol_id.cuda()\n",
        "        candidates = [[0, symbol_id, h_t, c_t, [get_symbol_id('<s>')]]]\n",
        "\n",
        "        # beam search\n",
        "        t = 0\n",
        "        while t < max_len - 1:\n",
        "            t += 1\n",
        "            tmp_candidates = []\n",
        "            end_flag = True\n",
        "            for score, last_id, h_t, c_t, id_seq in candidates:\n",
        "                if id_seq[-1] == get_symbol_id('</s>'):\n",
        "                    tmp_candidates.append([score, last_id, h_t, c_t, id_seq])\n",
        "                else:\n",
        "                    end_flag = False\n",
        "                    emb = self.B(last_id)\n",
        "                    output, h_t, c_t = self.forward_step(emb, h_t, c_t, mode=mode)\n",
        "                    output = output.squeeze(0).squeeze(0)\n",
        "                    # log softmax\n",
        "                    output = F.log_softmax(output)\n",
        "                    output, indices = torch.sort(output, descending=True)\n",
        "                    output = output[:beam_size]\n",
        "                    indices = indices[:beam_size]\n",
        "                    score_list = score + output\n",
        "                    for score, wid in zip(score_list, indices):\n",
        "                        tmp_candidates.append(\n",
        "                            [score, wid, h_t, c_t, id_seq + [int(wid.data.numpy())]]\n",
        "                        )\n",
        "            if end_flag:\n",
        "                break\n",
        "            # sort by normarized log probs and pick beam_size highest candidate\n",
        "            candidates = sorted(tmp_candidates,\n",
        "                                key=lambda x: -x[0].data.numpy()/len(x[-1]))[:beam_size]\n",
        "\n",
        "        return candidates[0][-1]"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xNW-xjWIjz7b",
        "outputId": "2a9ca23e-338a-48db-cc10-8b14acb96158"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "def sequence_mask(sequence_length, max_len=None):\n",
        "    if max_len is None:\n",
        "        max_len = sequence_length.data.max()\n",
        "    batch_size = sequence_length.size(0)\n",
        "    seq_range = torch.range(0, max_len - 1).long()\n",
        "    seq_range_expand = seq_range.unsqueeze(0).expand(batch_size, max_len)\n",
        "    seq_range_expand = Variable(seq_range_expand)\n",
        "    if sequence_length.is_cuda:\n",
        "        seq_range_expand = seq_range_expand.cuda()\n",
        "    seq_length_expand = (sequence_length.unsqueeze(1)\n",
        "                         .expand_as(seq_range_expand))\n",
        "    return seq_range_expand < seq_length_expand\n",
        "\n",
        "\n",
        "def masked_cross_entropy(logits, target, length):\n",
        "    length = Variable(length)\n",
        "    if torch.cuda.is_available():\n",
        "        length = length.cuda()\n",
        "\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        logits: A Variable containing a FloatTensor of size\n",
        "            (batch, max_len, num_classes) which contains the\n",
        "            unnormalized probability for each class.\n",
        "        target: A Variable containing a LongTensor of size\n",
        "            (batch, max_len) which contains the index of the true\n",
        "            class for each corresponding step.\n",
        "        length: A Variable containing a LongTensor of size (batch,)\n",
        "            which contains the length of each data in a batch.\n",
        "    Returns:\n",
        "        loss: An average loss value masked by the length.\n",
        "    \"\"\"\n",
        "\n",
        "    # logits_flat: (batch * max_len, num_classes)\n",
        "    logits_flat = logits.view(-1, logits.size(-1))\n",
        "    # log_probs_flat: (batch * max_len, num_classes)\n",
        "    log_probs_flat = F.log_softmax(logits_flat)\n",
        "    # target_flat: (batch * max_len, 1)\n",
        "    target_flat = target.view(-1, 1)\n",
        "    # losses_flat: (batch * max_len, 1)\n",
        "    losses_flat = -torch.gather(log_probs_flat, dim=1, index=target_flat)\n",
        "    # losses: (batch, max_len)\n",
        "    losses = losses_flat.view(*target.size())\n",
        "    # mask: (batch, max_len)\n",
        "    mask = sequence_mask(sequence_length=length, max_len=target.size(1))\n",
        "    losses = losses * mask.float()\n",
        "    loss = losses.sum() / length.float().sum()\n",
        "    return loss\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    length = torch.LongTensor([23, 21, 17])\n",
        "    length = Variable(length)\n",
        "\n",
        "    print(sequence_mask(length))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True, False, False],\n",
            "        [ True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True, False, False, False,\n",
            "         False, False, False]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "PO9e1eQUmIvB",
        "outputId": "0b660735-fb34-46d9-f885-90f2947226a2"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "import argparse\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "# from build_vocab import Vocab\n",
        "# from data_loader import get_data_loader\n",
        "# from data_loader import get_styled_data_loader\n",
        "# from models import EncoderCNN\n",
        "# from models import FactoredLSTM\n",
        "# from loss import masked_cross_entropy\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
        "\n",
        "\n",
        "def to_var(x, volatile=False):\n",
        "    if torch.cuda.is_available():\n",
        "        x = x.cuda()\n",
        "    return Variable(x, volatile=volatile)\n",
        "\n",
        "\n",
        "def eval_outputs(outputs, vocab):\n",
        "    # outputs: [batch, max_len - 1, vocab_size]\n",
        "    indices = torch.topk(outputs, 1)[1]\n",
        "    indices = indices.squeeze(2)\n",
        "    indices = indices.data\n",
        "    for i in range(len(indices)):\n",
        "        caption = [vocab.i2w[x] for x in indices[i]]\n",
        "        print(caption)\n",
        "\n",
        "\n",
        "def main( ):\n",
        "\n",
        "    model_path = '/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/'\n",
        "    if not os.path.exists(model_path):\n",
        "        os.makedirs(model_path)\n",
        "\n",
        "    # load vocablary\n",
        "    vocab_path='/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/vocab.pkl'\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    img_path = \"/content/drive/Shareddrives/BTP_group1/Flicker8k_Dataset/\"\n",
        "    factual_cap_path = \"/content/drive/Shareddrives/BTP_group1/Flickr8k_text/Flickr8k.token.txt\"\n",
        "    humorous_cap_path = \"/content/drive/MyDrive/new_codes_github/FlickrStyle_v0.9/FlickrStyle_v0.9/humor/funny_train.txt\"\n",
        "    # img_path = args.img_path\n",
        "    # factual_cap_path = args.factual_caption_path\n",
        "    # humorous_cap_path = args.humorous_caption_path\n",
        "    caption_batch_size=64\n",
        "    language_batch_size=96\n",
        "    emb_dim=300\n",
        "    hidden_dim=512\n",
        "    factored_dim=512\n",
        "    lr_caption=0.0002\n",
        "    lr_language=0.0005\n",
        "    epoch_num=30\n",
        "    log_step_caption=50\n",
        "    log_step_language=10\n",
        "\n",
        "\n",
        "\n",
        "    # import data_loader\n",
        "    data_loader = get_data_loader(img_path, factual_cap_path, vocab,caption_batch_size, shuffle=True)\n",
        "    # print(\" factual_cap_path\", factual_cap_path)\n",
        "    styled_data_loader = get_styled_data_loader(humorous_cap_path, vocab,\n",
        "                                                language_batch_size,\n",
        "                                                shuffle=True)\n",
        "    # img_path= next(iter(data_loader))\n",
        "    # print(f\"Feature batch shape: {img_path.size()}\")\n",
        "    # print(f\"Labels batch shape: {factual_cap_path.size()}\")\n",
        "    \n",
        "    emb_dim = emb_dim\n",
        "    hidden_dim = hidden_dim\n",
        "    factored_dim = factored_dim\n",
        "    vocab_size = len(vocab)\n",
        "    encoder = EncoderCNN(emb_dim)\n",
        "    decoder = FactoredLSTM(emb_dim, hidden_dim, factored_dim, vocab_size)\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        encoder = encoder.cuda()\n",
        "        decoder = decoder.cuda()\n",
        "\n",
        "    # loss and optimizer\n",
        "    criterion = masked_cross_entropy\n",
        "    cap_params = list(decoder.parameters()) + list(encoder.A.parameters())\n",
        "    lang_params = list(decoder.parameters())\n",
        "    optimizer_cap = torch.optim.Adam(cap_params, lr=lr_caption)\n",
        "    optimizer_lang = torch.optim.Adam(lang_params, lr=lr_language)\n",
        "\n",
        "    # train\n",
        "    total_cap_step = len(data_loader)\n",
        "    print(\"total_cap_step\", total_cap_step)\n",
        "    total_lang_step = len(styled_data_loader)\n",
        "    print(\"total_lang_step\", total_lang_step)\n",
        "    epoch_num = epoch_num\n",
        "    for epoch in range(epoch_num):\n",
        "        # caption\n",
        "        # for i, (images, captions, lengths) in enumerate(data_loader):\n",
        "        for i, (images, captions, lengths) in enumerate(data_loader):\n",
        "            images = to_var(images, volatile=True)\n",
        "            captions = to_var(captions.long())\n",
        "\n",
        "            # forward, backward and optimize\n",
        "            decoder.zero_grad()\n",
        "            encoder.zero_grad()\n",
        "            features = encoder(images.float())\n",
        "            outputs = decoder(captions, features, mode=\"factual\")\n",
        "            loss = criterion(outputs[:, 1:, :].contiguous(),\n",
        "                             captions[:, 1:].contiguous(), lengths - 1)\n",
        "            loss.backward()\n",
        "            optimizer_cap.step()\n",
        "\n",
        "            # print log\n",
        "            if i % log_step_caption == 0:\n",
        "                print(\"Epoch [%d/%d], CAP, Step [%d/%d], Loss: %.4f\"\n",
        "                      % (epoch+1, epoch_num, i, total_cap_step,\n",
        "                          loss.data.mean()))\n",
        "\n",
        "        eval_outputs(outputs, vocab)\n",
        "\n",
        "        # language\n",
        "        for i, (captions, lengths) in enumerate(styled_data_loader):\n",
        "            captions = to_var(captions.long())\n",
        "\n",
        "            # forward, backward and optimize\n",
        "            decoder.zero_grad()\n",
        "            outputs = decoder(captions, mode='humorous')\n",
        "            loss = criterion(outputs, captions[:, 1:].contiguous(), lengths-1)\n",
        "            loss.backward()\n",
        "            optimizer_lang.step()\n",
        "\n",
        "            # print log\n",
        "            if i % log_step_language == 0:\n",
        "                print(\"Epoch [%d/%d], LANG, Step [%d/%d], Loss: %.4f\"\n",
        "                      % (epoch+1, epoch_num, i, total_lang_step,\n",
        "                          loss.data.mean()))\n",
        "\n",
        "        # save models\n",
        "        torch.save(decoder.state_dict(),\n",
        "                   os.path.join(model_path, 'decoder-%d.pkl' % (epoch + 1,)))\n",
        "\n",
        "        torch.save(encoder.state_dict(),\n",
        "                   os.path.join(model_path, 'encoder-%d.pkl' % (epoch + 1,)))\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # parser = argparse.ArgumentParser(\n",
        "    #         description='StyleNet: Generating Attractive Visual Captions \\\n",
        "    #                     with Styles')\n",
        "    # parser.add_argument('--model_path', type=str, default='pretrained_models',\n",
        "    #                     help='path for saving trained models')\n",
        "    # parser.add_argument('--vocab_path', type=str, default='./data/vocab.pkl',\n",
        "    #                     help='path for vocabrary')\n",
        "    # parser.add_argument('--img_path', type=str,\n",
        "    #                     default='./data/flickr7k_images',\n",
        "    #                     help='path for train images directory')\n",
        "    # parser.add_argument('--factual_caption_path', type=str,\n",
        "    #                     default='./data/factual_train.txt',\n",
        "    #                     help='path for factual caption file')\n",
        "    # parser.add_argument('--humorous_caption_path', type=str,\n",
        "    #                     default='./data/humor/funny_train.txt',\n",
        "    #                     help='path for humorous caption file')\n",
        "    # parser.add_argument('--romantic_caption_path', type=str,\n",
        "    #                     default='./data/romantic/romanntic_train.txt',\n",
        "    #                     help='path for romantic caption file')\n",
        "    # parser.add_argument('--caption_batch_size', type=int, default=64,\n",
        "    #                     help='mini batch size for caption model training')\n",
        "    # parser.add_argument('--language_batch_size', type=int, default=96,\n",
        "    #                     help='mini batch size for language model training')\n",
        "    # parser.add_argument('--emb_dim', type=int, default=300,\n",
        "    #                     help='embedding size of word, image')\n",
        "    # parser.add_argument('--hidden_dim', type=int, default=512,\n",
        "    #                     help='hidden state size of factored LSTM')\n",
        "    # parser.add_argument('--factored_dim', type=int, default=512,\n",
        "    #                     help='size of factored matrix')\n",
        "    # parser.add_argument('--lr_caption', type=int, default=0.0002,\n",
        "    #                     help='learning rate for caption model training')\n",
        "    # parser.add_argument('--lr_language', type=int, default=0.0005,\n",
        "    #                     help='learning rate for language model training')\n",
        "    # parser.add_argument('--epoch_num', type=int, default=30)\n",
        "    # parser.add_argument('--log_step_caption', type=int, default=50,\n",
        "    #                     help='steps for print log while train caption model')\n",
        "    # parser.add_argument('--log_step_language', type=int, default=10,\n",
        "    #                     help='steps for print log while train language model')\n",
        "    # args = parser.parse_args()\n",
        "\n",
        "    main()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total_cap_step 633\n",
            "total_lang_step 73\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
            "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:129: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:130: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:42: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: UserWarning: torch.range is deprecated and will be removed in a future release because its behavior is inconsistent with Python's range builtin. Instead, use torch.arange, which produces values in [start, end).\n",
            "  # Remove the CWD from sys.path while we load stuff.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch [1/30], CAP, Step [0/633], Loss: 9.7000\n",
            "Epoch [1/30], CAP, Step [50/633], Loss: 5.0705\n",
            "Epoch [1/30], CAP, Step [100/633], Loss: 4.6606\n",
            "Epoch [1/30], CAP, Step [150/633], Loss: 4.3959\n",
            "Epoch [1/30], CAP, Step [200/633], Loss: 4.1278\n",
            "Epoch [1/30], CAP, Step [250/633], Loss: 4.2035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9cc2e26dae67>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    185\u001b[0m     \u001b[0;31m# args = parser.parse_args()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-9cc2e26dae67>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0;31m# caption\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0;31m# for i, (images, captions, lengths) in enumerate(data_loader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m             \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvolatile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0mcaptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_var\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-7d13da2c3b56>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, ix)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgname_caption_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, **plugin_args)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/skimage/io/_plugins/imageio_plugin.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimageio_imread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/content/drive/Shareddrives/BTP_group1/Flicker8k_Dataset/2258277193_586949ec62.jpg.1'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqPzddAAnUHo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}