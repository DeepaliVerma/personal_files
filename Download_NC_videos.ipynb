{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled37.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN1ptr+acXqfrN8ViswVvYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DeepaliVerma/personal_files/blob/main/Download_NC_videos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NytxOy1mzG0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eF1QhD4Pmzhr"
      },
      "source": [
        "#Download archieve videos\n",
        "https://www.geeksforgeeks.org/downloading-files-web-using-python/#:~:text=Downloading%20Videos&text=So%2C%20we%20first%20scrape%20the,the%20videos%20one%20by%20one.&text=download%20each%20video%20manually.&text=all%20the%20links%20and%20then%20download%20videos.&text=print%20(%20%22All%20videos%20downloaded!%22%20)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dTbgBLSmm8_"
      },
      "source": [
        "#for all video (final one)\n",
        "import requests\n",
        "#from bs4 import BeautifulSoup\n",
        "\n",
        "# name=\"CNN_20121027_030000_Erin_Burnett_OutFront\"\n",
        "def get_video_links():\n",
        "  #archive_url =\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=0&end=60&ignore=x.mp4\"\n",
        "  string =\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=0&end=60&ignore=x.mp4\"\n",
        "  string1=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=60&end=120&ignore=x.mp4\"\n",
        "  string2=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=120&end=180&ignore=x.mp4\"\n",
        "  string3=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=180&end=240&ignore=x.mp4\"\n",
        "  string4=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=240&end=300&ignore=x.mp4\"\n",
        "  string5=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=300&end=360&ignore=x.mp4\"\n",
        "  string6=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=360&end=420&ignore=x.mp4\"\n",
        "  string7=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=420&end=480&ignore=x.mp4\"\n",
        "  string8=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=480&end=540&ignore=x.mp4\"\n",
        "  string9=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=540&end=600&ignore=x.mp4\"\n",
        "  string10=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=600&end=660&ignore=x.mp4\"\n",
        "  string11=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=660&end=720&ignore=x.mp4\"\n",
        "  archive_url =[string, string1, string2, string3, string4, string5, string6, string7, string8, string9, string10, string11]\n",
        "\t# create response object\n",
        "  # Printing list\n",
        "  print (\"Original list is : \" + str(archive_url))\n",
        "  # r = requests.get(archive_url)\n",
        " \n",
        "\n",
        "\t# filter the link sending with .mp4\n",
        "\t#video_links = [archive_url  for link in links if link['href'].endswith('mp4')]\n",
        "  video_links = archive_url\n",
        "  return video_links\n",
        "\n",
        "def download_video_series(video_links):\n",
        "  i=0\n",
        "  for link in video_links:\n",
        "    file_name = name+\"_%d.mp4\"%(i+1)\n",
        "    links=\"\"\n",
        "    with open('labels1.txt', 'a') as file1:\n",
        "      # for line in file_name:\n",
        "        #file1.write('\\n'.join(video_links))\n",
        "        links=links.join(link)\n",
        "        file1.write(links)\n",
        "        file1.write('\\n')\n",
        "    r = requests.get(link, stream = True)\n",
        "    with open(file_name, 'wb') as f:\n",
        "      for chunk in r.iter_content(chunk_size = 1024*1024):\n",
        "        if chunk:\n",
        "          f.write(chunk)\n",
        "    print( \"Downloading file:%s\"%link)\n",
        "    i=i+1\n",
        "\n",
        "  print (\"All videos downloaded!\")\n",
        "  return\n",
        "if __name__ == \"__main__\":\n",
        "# Opening file\n",
        "  file1 = open('archive_show.txt', 'r')\n",
        "  count = 0\n",
        " \n",
        "# Using for loop\n",
        "  print(\"Using for loop\")\n",
        "  for line in file1:\n",
        "    count += 1\n",
        "    print(\"Line{}: {}\".format(count, line.strip()))\n",
        "    name=line.strip()\n",
        "    video_links = get_video_links()\n",
        "    download_video_series(video_links)\n",
        " \n",
        "# Closing files\n",
        "  file1.close()\n",
        "\t# getting all video links\n",
        "  # with open('archive_show.txt') as f:\n",
        "  #   name = f.readline()\n",
        "  # #   print(name.strip())\n",
        "  # video_links = get_video_links()\n",
        "  #   # download all videos\n",
        "  # download_video_series(video_links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YD1bzFMbnPRj"
      },
      "source": [
        "##For single program"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgNhR4xFnPBt"
      },
      "source": [
        "#multiple files download\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "'''\n",
        "URL of the archive web-page which provides link to\n",
        "all video lectures. It would have been tiring to\n",
        "download each video manually.\n",
        "In this example, we first crawl the webpage to extract\n",
        "all the links and then download videos.\n",
        "'''\n",
        "# specify the URL of the archive here\n",
        "#archive_url = \"https://ia802204.us.archive.org/8/items/CNN_20100112_010000_Campbell_Brown/CNN_20100112_010000_Campbell_Brown.mp4?start=0&end=60&ignore=x.mp4\"\n",
        "name = \"CNN_20120607_230000_Erin_Burnett_OutFront\"\n",
        "\n",
        "\n",
        "def get_video_links():\n",
        "  #archive_url =\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=0&end=60&ignore=x.mp4\"\n",
        "  string =\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=0&end=60&ignore=x.mp4\"\n",
        "  string1=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=60&end=120&ignore=x.mp4\"\n",
        "  string2=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=120&end=180&ignore=x.mp4\"\n",
        "  string3=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=180&end=240&ignore=x.mp4\"\n",
        "  string4=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=240&end=300&ignore=x.mp4\"\n",
        "  string5=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=300&end=360&ignore=x.mp4\"\n",
        "  string6=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=360&end=420&ignore=x.mp4\"\n",
        "  string7=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=420&end=480&ignore=x.mp4\"\n",
        "  string8=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=480&end=540&ignore=x.mp4\"\n",
        "  string9=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=540&end=600&ignore=x.mp4\"\n",
        "  string10=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=600&end=660&ignore=x.mp4\"\n",
        "  string11=\"https://ia802204.us.archive.org/8/items/\"+name+\"/\"+name+\".mp4?start=660&end=720&ignore=x.mp4\"\n",
        "  archive_url =[string, string1, string2, string3, string4, string5, string6, string7, string8, string9, string10, string11]\n",
        "\t# create response object\n",
        "  # Printing list\n",
        "  print (\"Original list is : \" + str(archive_url))\n",
        "  # r = requests.get(archive_url)\n",
        " \n",
        "\n",
        "\t# filter the link sending with .mp4\n",
        "\t#video_links = [archive_url  for link in links if link['href'].endswith('mp4')]\n",
        "  video_links = archive_url\n",
        "  return video_links\n",
        "\n",
        "\n",
        "def download_video_series(video_links):\n",
        "  i=0\n",
        "  for link in video_links:\n",
        "    file_name = name+\"_%d.mp4\"%(i+1)\n",
        "    with open('labels.txt', 'a') as file1:\n",
        "      # for line in file_name:\n",
        "        file1.write('\\n'.join(video_links))\n",
        "        file1.write('\\n')\n",
        "    r = requests.get(link, stream = True)\n",
        "    with open(file_name, 'wb') as f:\n",
        "      for chunk in r.iter_content(chunk_size = 1024*1024):\n",
        "        if chunk:\n",
        "          f.write(chunk)\n",
        "    print( \"Downloading file:%s\"%link)\n",
        "    i=i+1\n",
        "\n",
        "  print (\"All videos downloaded!\")\n",
        "  return\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\t# getting all video links\n",
        "\tvideo_links = get_video_links()\n",
        "\n",
        "\t# download all videos\n",
        "  # with open('labels.txt', 'w') as file1:\n",
        "  #     # for line in file_name:\n",
        "  #   file1.writelines(link)\n",
        "  #   file1.write('\\n')\n",
        "\tdownload_video_series(video_links)\n",
        "\t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaAh49dpneFc"
      },
      "source": [
        "# For NewsVideoDataset Captioning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1qvO7nGndnC"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bgbTtKn5nucQ"
      },
      "source": [
        "!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWAF3rQsnvPF"
      },
      "source": [
        "!sudo chmod a+rx /usr/local/bin/youtube-dl"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJE4AhWVnyzF"
      },
      "source": [
        "!git clone https://github.com/SpencerWhitehead/NewsVideoDataset.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwxCbyGpn1LP"
      },
      "source": [
        "!youtube-dl -o \"META_DATA_FILE_PATH/vid.%(id)s.%(ext)s\" --batch-file urls.txt --restrict-filenames --write-info-json --recode-video mp4 --sleep-interval 20 --max-sleep-interval 50"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}